{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekyaMadiReddy/Alekya_INFO5731_Fall2023/blob/main/MadiReddy_exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3503bc52-f0bb-4890-8d3c-0dc8305512c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nThe categorization of articles of Coding\\xa0research into categories like \"Python,\" \"C++,\" \"Dbms,\" \"Java,\" and \"Pycharm\" might be an intriguing text classification challenge in coding. Researchers might easily find suitable publications for their particular study interests with the help of this assignment.\\n\\nThe following five features may be used to create a machine learning model for this task:\\n\\nFeatures of the Bag of Words (BoW): BoW features show how frequently each word appears in the text. These characteristics can aid in identifying the most prevalent words connected to each study category.\\nTerm frequency-inverse document frequency, or TF-IDF Features: When comparing a word\\'s relevance within a text to its importance throughout the entire corpus, TF-IDF is used. These qualities might draw attention to terms that are especially pertinent to a given field of study.\\n\\nFeatures of N-grams: N-grams are groups of neighboring words or letters. Important textual phrases or patterns that may be indicative of the study category can be extracted using n-grams.\\n\\nWord Embeddings: Word embeddings, like GloVe or Word2Vec, may turn words into detailed vector representations. The context of terms used in the text may be understood using these embeddings, which record the semantic links between words.\\n\\nTopic Modeling Features: Topic modeling techniques like Latent Dirichlet Allocation (LDA) can identify the underlying topics in a document. These features can help categorize research articles based on the topics they cover\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "The categorization of articles of CodingÂ research into categories like \"Python,\" \"C++,\" \"Dbms,\" \"Java,\" and \"Pycharm\" might be an intriguing text classification challenge in coding. Researchers might easily find suitable publications for their particular study interests with the help of this assignment.\n",
        "\n",
        "The following five features may be used to create a machine learning model for this task:\n",
        "\n",
        "Features of the Bag of Words (BoW): BoW features show how frequently each word appears in the text. These characteristics can aid in identifying the most prevalent words connected to each study category.\n",
        "Term frequency-inverse document frequency, or TF-IDF Features: When comparing a word's relevance within a text to its importance throughout the entire corpus, TF-IDF is used. These qualities might draw attention to terms that are especially pertinent to a given field of study.\n",
        "\n",
        "Features of N-grams: N-grams are groups of neighboring words or letters. Important textual phrases or patterns that may be indicative of the study category can be extracted using n-grams.\n",
        "\n",
        "Word Embeddings: Word embeddings, like GloVe or Word2Vec, may turn words into detailed vector representations. The context of terms used in the text may be understood using these embeddings, which record the semantic links between words.\n",
        "\n",
        "Topic Modeling Features: Topic modeling techniques like Latent Dirichlet Allocation (LDA) can identify the underlying topics in a document. These features can help categorize research articles based on the topics they cover\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1aa443-a281-4a89-a1d4-871e6ae24738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Features:\n",
            "[[0 1 0 2 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0]\n",
            " [1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0]]\n",
            "\n",
            "TF-IDF Features:\n",
            "[[0.         0.32603555 0.         0.6520711  0.20810427 0.\n",
            "  0.32603555 0.         0.32603555 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.32603555 0.         0.         0.\n",
            "  0.         0.         0.32603555]\n",
            " [0.         0.         0.         0.         0.21242036 0.\n",
            "  0.         0.         0.         0.         0.         0.33279753\n",
            "  0.         0.         0.33279753 0.         0.         0.33279753\n",
            "  0.         0.         0.         0.         0.33279753 0.\n",
            "  0.         0.33279753 0.         0.33279753 0.         0.33279753\n",
            "  0.33279753 0.2623814  0.        ]\n",
            " [0.31577018 0.         0.31577018 0.         0.20155202 0.31577018\n",
            "  0.         0.31577018 0.         0.31577018 0.         0.\n",
            "  0.         0.         0.         0.         0.31577018 0.\n",
            "  0.         0.31577018 0.         0.31577018 0.         0.\n",
            "  0.31577018 0.         0.         0.         0.         0.\n",
            "  0.         0.24895684 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.35355339 0.\n",
            "  0.35355339 0.35355339 0.         0.35355339 0.         0.\n",
            "  0.35355339 0.         0.35355339 0.         0.         0.35355339\n",
            "  0.         0.         0.         0.         0.35355339 0.\n",
            "  0.         0.         0.        ]]\n",
            "\n",
            "N-grams Features:\n",
            "[[0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0\n",
            "  0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            "  0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Topic Modeling Features (LDA):\n",
            "[[0.94160308 0.05839692]\n",
            " [0.95062716 0.04937284]\n",
            " [0.04805537 0.95194463]\n",
            " [0.05800346 0.94199654]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Sample coding research articles\n",
        "documents = [\n",
        "    \"Blockchain Technology Demystified: Building Your First Blockchain Application\",\n",
        "    \"Getting Started with React: Building User Interfaces the Modern Way\",\n",
        "    \"Coding for IoT: Building Smart Devices with Arduino and Raspberry Pi\",\n",
        "    \"Introduction to Functional Programming in Haskell: A Paradigm Shift\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "nltk.download('punkt')\n",
        "tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Bag of Words (BoW) features\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# TF-IDF features\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_features = vectorizer_tfidf.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# N-grams features (bi-grams and tri-grams)\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 3))\n",
        "ngrams_features = vectorizer_ngrams.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# Topic modeling features using LDA\n",
        "lda = LatentDirichletAllocation(n_components=2)\n",
        "lda_features = lda.fit_transform(bow_features)\n",
        "\n",
        "# Word embeddings (using pre-trained Word2Vec or GloVe embeddings)\n",
        "# You would typically load pre-trained embeddings and transform the text into vectors.\n",
        "\n",
        "# Display the extracted features\n",
        "print(\"BoW Features:\")\n",
        "print(bow_features.toarray())\n",
        "\n",
        "print(\"\\nTF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "\n",
        "print(\"\\nN-grams Features:\")\n",
        "print(ngrams_features.toarray())\n",
        "\n",
        "print(\"\\nTopic Modeling Features (LDA):\")\n",
        "print(lda_features)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define the target labels (research categories)\n",
        "labels = [\"Python\", \"Supplychain\", \"Java\", \"Dbms\"]\n",
        "\n",
        "# Assuming you have a target label for each document, e.g., [0, 1, 2, 3]\n",
        "\n",
        "# Feature selection using chi-squared statistic\n",
        "k_best = SelectKBest(score_func=chi2, k='all')\n",
        "selected_features = k_best.fit_transform(bow_features, labels)\n",
        "\n",
        "# Get the indices of selected features in descending order of importance\n",
        "sorted_feature_indices = (-k_best.scores_).argsort()\n",
        "\n",
        "# List the features in descending order of importance\n",
        "sorted_features = [vectorizer_bow.get_feature_names_out()[i] for i in sorted_feature_indices]\n",
        "\n",
        "print(\"Top Features in Descending Order of Importance:\")\n",
        "print(sorted_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtDyyuq_Xy_i",
        "outputId": "891756cd-e311-48e5-e14a-b7d8a1f719ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features in Descending Order of Importance:\n",
            "['blockchain', 'and', 'way', 'user', 'to', 'the', 'technology', 'started', 'smart', 'shift', 'react', 'raspberry', 'programming', 'pi', 'paradigm', 'modern', 'iot', 'introduction', 'interfaces', 'in', 'haskell', 'getting', 'functional', 'for', 'first', 'devices', 'demystified', 'coding', 'arduino', 'application', 'your', 'with', 'building']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFmhOx_xXEpd",
        "outputId": "73a158d8-7036-42b2-c8bb-dadba444825c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FdbI4QlXNX-",
        "outputId": "4c9542a0-d9ed-42bd-876c-adf067d0fec2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "# Your query\n",
        "query = \"Introduction to Functional Programming in Haskell: A Paradigm Shift\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose other BERT variants as well\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Encode the query and documents\n",
        "query_encoding = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "document_encodings = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Get BERT embeddings for the query and documents\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**query_encoding).last_hidden_state.mean(dim=1)\n",
        "    document_embeddings = model(**document_encodings).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate cosine similarity between the query and documents\n",
        "similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "\n",
        "# Rank documents by similarity in descending order\n",
        "ranked_documents = [(document, similarity) for document, similarity in zip(documents, similarities)]\n",
        "ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked documents\n",
        "for i, (document, similarity) in enumerate(ranked_documents, start=1):\n",
        "    print(f\"Rank {i}: Similarity = {similarity:.4f}\")\n",
        "    print(document)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dttMjCfWXV95",
        "outputId": "569ee8bc-1162-46dd-a2aa-73515aac58dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1: Similarity = 0.9539\n",
            "Introduction to Functional Programming in Haskell: A Paradigm Shift\n",
            "\n",
            "Rank 2: Similarity = 0.6811\n",
            "Getting Started with React: Building User Interfaces the Modern Way\n",
            "\n",
            "Rank 3: Similarity = 0.6350\n",
            "Coding for IoT: Building Smart Devices with Arduino and Raspberry Pi\n",
            "\n",
            "Rank 4: Similarity = 0.6158\n",
            "Blockchain Technology Demystified: Building Your First Blockchain Application\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}